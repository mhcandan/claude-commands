Assess team readiness and maturity for AI-native software development.

## Team Assessment Framework

Based on McKinsey research: "Simply adopting AI tools is not enough. Companies must rethink how they structure teams and build software in an AI-forward world."

**Key Finding**: 15 percentage point performance gap between top and bottom performers.

### What High Performers Look Like

Top performers achieve:
- 16-30% improvement in team productivity, customer experience, time to market
- 31-45% improvement in software quality
- Higher artifact consistency and quality
- Shorter sprint cycles
- Smaller team sizes
- Higher customer satisfaction scores

### Two Key Shifts Assessment

#### Shift 1: End-to-End PDLC Implementation
Top performers are 6-7x more likely to scale to 4+ use cases across the PDLC.

Rate your team's AI usage across stages:
| Stage | No AI | Experimenting | Scaled | AI-Native |
|-------|-------|---------------|--------|-----------|
| Requirements/Design | | | | |
| Coding | | | | |
| Code Review | | | | |
| Testing | | | | |
| Deployment | | | | |
| Monitoring | | | | |

**Target**: 4+ stages at "Scaled" or better (only 10% of bottom performers achieve this)

#### Shift 2: AI-Native Roles
Assess role evolution:

**Product Managers** - Are they:
- [ ] Spending less time on feature delivery?
- [ ] More involved in design and prototyping?
- [ ] Engaged in QA and responsible AI practices?
- [ ] Building AI-specific skills (governance, customer insight)?

**Software Engineers** - Are they:
- [ ] Developing full-stack fluency?
- [ ] Skilled at structured spec communication?
- [ ] Understanding architectural trade-offs?
- [ ] Operating as orchestrators of AI agents?

**New Skills Required**:
- Problem framing and intent specification
- Prompt engineering (requires intensive training)
- Managing async agent workflows
- "Engineering manager for AI juniors" mindset

### Three Critical Enablers Assessment

#### Enabler 1: Upskilling (57% top vs 20% bottom)
| Method | Available | Used | Effective |
|--------|-----------|------|-----------|
| On-demand courses | | | |
| Hands-on workshops | | | |
| One-on-one coaching | | | |
| AI guilds/centers of enablement | | | |
| Sprint-integrated learning | | | |

**Best Practice**: Training mirrors real development work, personalized by role, continuous not one-off.

#### Enabler 2: Impact Measurement (Track Outcomes, Not Just Adoption)
| Metric Type | Currently Tracked | Quality |
|-------------|-------------------|---------|
| **Outcome Metrics** | | |
| Quality improvements | | |
| Speed gains | | |
| Customer satisfaction | | |
| **Avoid These (Weak Proxies)** | | |
| % of code generated by AI | | |
| Tool usage frequency | | |
| Code acceptance rates | | |

**Quote from Sonar CEO**: "Lines of code or AI contribution percentages don't reveal whether the output is secure, maintainable, or even useful."

#### Enabler 3: Change Management
| Practice | Yes | No |
|----------|-----|-----|
| AI goals linked to developer reviews | | |
| AI goals linked to PM reviews | | |
| Incentives tied to behaviors, not just usage | | |
| Teams connect AI work to broader outcomes | | |

**Target**: 80% of top performers link gen-AI goals to reviews (vs 10% of bottom)

### Cursor-Style Workflow Checklist

Modern AI-native workflow elements:
- [ ] Plan Mode: Plot changes before implementation
- [ ] Background agents: Parallel task execution
- [ ] AI code review (like Bugbot): Pre-human validation
- [ ] DRI per release: Dedicated responsible individual
- [ ] Small pods with cross-team projects
- [ ] Designers prototype directly in code
- [ ] Business teams query product data via platform

## Output Format

```
## Team AI-Readiness Assessment

### Overall Maturity Level: [Beginner/Developing/Advanced/AI-Native]
### Performance Tier Prediction: [Bottom/Middle/Top]

### Two Shifts Scorecard

| Shift | Score | Status | Gap |
|-------|-------|--------|-----|
| End-to-End PDLC | X/10 | [status] | [gap to top performer] |
| AI-Native Roles | X/10 | [status] | [gap to top performer] |

### Three Enablers Scorecard

| Enabler | Score | Status | Priority |
|---------|-------|--------|----------|
| Upskilling | X/10 | [status] | [priority] |
| Impact Measurement | X/10 | [status] | [priority] |
| Change Management | X/10 | [status] | [priority] |

### PDLC Coverage Map
```
Design → Code → Review → Test → Deploy → Monitor
 [X]     [X]     [X]      [X]     [X]      [X]
```
Use cases at scale: [count]/6

### Role Evolution Status

| Role | Traditional | Transitioning | AI-Native |
|------|-------------|---------------|-----------|

### Key Gaps Identified
1. [Gap]: [Impact] - [Recommendation]

### Improvement Roadmap
1. **Immediate** (High Impact, Low Effort): [actions]
2. **Short-term** (Build Foundation): [actions]
3. **Long-term** (Transform Operating Model): [actions]

### Expected Outcomes If Gaps Addressed
- Productivity: +[X]%
- Quality: +[X]%
- Time to Market: +[X]%
```

Provide information about your team structure, current AI tool usage, training programs, and metrics tracked for assessment.
